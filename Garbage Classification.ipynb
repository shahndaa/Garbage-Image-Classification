{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d018353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from PIL import Image\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcec84cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 6 classes.\n",
      "Found 0 images belonging to 6 classes.\n",
      "Class indices: {'cardboard': 0, 'glass': 1, 'metal': 2, 'paper': 3, 'plastic': 4, 'trash': 5}\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"E:/Codes/Garbage Classification/dataset-resized\"\n",
    "classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    classes=classes,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    classes=classes,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "print(\"Class indices:\", train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55427240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: {'trash': 137, 'cardboard': 403, 'glass': 501, 'metal': 410, 'paper': 594, 'plastic': 482}\n",
      "Found 2022 images belonging to 2 classes.\n",
      "Found 505 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "class_counts = Counter()\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    if os.path.basename(root) in ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']:\n",
    "        class_counts[os.path.basename(root)] += len(files)\n",
    "\n",
    "print(\"Class distribution:\", dict(class_counts))\n",
    "total = sum(class_counts.values())\n",
    "class_weights = {\n",
    "    0: total / (6 * class_counts['cardboard']),\n",
    "    1: total / (6 * class_counts['glass']),\n",
    "    2: total / (6 * class_counts['metal']),\n",
    "    3: total / (6 * class_counts['paper']),\n",
    "    4: total / (6 * class_counts['plastic']),\n",
    "    5: total / (6 * class_counts['trash'])\n",
    "}\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aa98e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e6072f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'cardboard' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcardboard\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m403\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m501\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrash\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m137\u001b[39m\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m     10\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(class_counts\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m---> 11\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mlist\u001b[39m(train_generator\u001b[38;5;241m.\u001b[39mclass_indices\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mcls\u001b[39m): total \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(class_counts) \u001b[38;5;241m*\u001b[39m count)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, count \u001b[38;5;129;01min\u001b[39;00m class_counts\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     16\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     17\u001b[0m     train_generator,\n\u001b[0;32m     18\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     ]\n\u001b[0;32m     25\u001b[0m )\n",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcardboard\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m403\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m501\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrash\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m137\u001b[39m\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m     10\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(class_counts\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     11\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28mlist\u001b[39m(train_generator\u001b[38;5;241m.\u001b[39mclass_indices\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mcls\u001b[39m): total \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(class_counts) \u001b[38;5;241m*\u001b[39m count)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, count \u001b[38;5;129;01min\u001b[39;00m class_counts\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     16\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     17\u001b[0m     train_generator,\n\u001b[0;32m     18\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     ]\n\u001b[0;32m     25\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: 'cardboard' is not in list"
     ]
    }
   ],
   "source": [
    "class_counts = {\n",
    "    'cardboard': 403,\n",
    "    'glass': 501,\n",
    "    'metal': 410,\n",
    "    'paper': 594,\n",
    "    'plastic': 482,\n",
    "    'trash': 137\n",
    "}\n",
    "\n",
    "total = sum(class_counts.values())\n",
    "class_weights = {\n",
    "    list(train_generator.class_indices.keys()).index(cls): total / (len(class_counts) * count)\n",
    "    for cls, count in class_counts.items()\n",
    "}\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=30,\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=3)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(val_generator)\n",
    "print(f\"Validation Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Validation Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
    "\n",
    "def predict_garbage(image_path, confidence_threshold=0.7):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB').resize((224, 224))\n",
    "        img_array = tf.keras.applications.mobilenetv2.preprocess_input(\n",
    "            np.expand_dims(np.array(img), axis=0))\n",
    "        \n",
    "        pred = model.predict(img_array)[0]\n",
    "        pred_idx = np.argmax(pred)\n",
    "        confidence = np.max(pred)\n",
    "        \n",
    "        if confidence < confidence_threshold:\n",
    "            top3 = np.argsort(pred)[-3:][::-1]\n",
    "            return {\n",
    "                'status': 'uncertain',\n",
    "                'predictions': [(class_names[i], float(pred[i])) for i in top3]\n",
    "            }\n",
    "            \n",
    "        return {\n",
    "            'status': 'confident',\n",
    "            'prediction': class_names[pred_idx],\n",
    "            'confidence': float(confidence)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'status': 'error', 'message': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e631e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict_garbage(\"recyclable/plastic/plastic14.jpg\")\n",
    "\n",
    "if result['status'] == 'confident':\n",
    "    print(f\"Prediction: {result['prediction']} ({result['confidence']:.1%})\")\n",
    "elif result['status'] == 'uncertain':\n",
    "    print(\"Top predictions:\")\n",
    "    for pred, conf in result['predictions']:\n",
    "        print(f\"- {pred}: {conf:.1%}\")\n",
    "else:\n",
    "    print(f\"Error: {result['message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"garbage_classifier_v2.h5\")\n",
    "print(\"Model saved successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
